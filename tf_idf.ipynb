{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLgUsEXkAkmpLL3dvYvmJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/covid19-twitter-usa-restoring/blob/main/tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vb0n7596einp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New York City"
      ],
      "metadata": {
        "id": "o7WASehkJnG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import re\n",
        "\n",
        "# Function to filter out non-English words\n",
        "def is_english_word(word):\n",
        "    return bool(re.match('^[a-zA-Z]+$', word))\n",
        "\n",
        "# Words to be excluded from TF-IDF calculation\n",
        "exclude_words = ['january', 'jan', 'february', 'feb', 'march', 'mar', 'april', 'apr', 'may','june', 'jun', 'july', 'jul', 'august', 'aug', 'september', 'sep', 'october', 'oct', 'november', 'nov','december', 'dec', 'holiday', 'holidays']\n",
        "\n",
        "# Function to filter out specified words\n",
        "def exclude_specific_words(text):\n",
        "    words = text.split()\n",
        "    return [word for word in words if is_english_word(word) and word.lower() not in exclude_words]\n",
        "\n",
        "# min_df=0.10: Exclude the bottom 10% of low-frequency terms\n",
        "# max_df=0.90: Exclude the top 90% of high-frequency terms\n",
        "# ngram_range=(1, 2): Include phrases of up to 2 characters\n",
        "count_vectorizer = CountVectorizer(stop_words='english', input='filename', ngram_range=(1, 1), max_df=0.99)\n",
        "\n",
        "# Corrected path for the files\n",
        "base_path = '/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/new_york_city/tf-idf/total_period_text/'\n",
        "files = [base_path + content for content in sorted(os.listdir(base_path))]\n",
        "\n",
        "# Filter out non-English words and specified words from the features\n",
        "count_vectorizer.set_params(tokenizer=lambda text: exclude_specific_words(text))\n",
        "\n",
        "feature_vectors = count_vectorizer.fit_transform(files)\n",
        "print(\"Feature Vectors Shape:\", feature_vectors.shape)\n",
        "\n",
        "terms = count_vectorizer.get_feature_names_out()\n",
        "print(\"Number of Terms:\", len(terms))\n",
        "\n",
        "# Setting sublinear_tf to True results in logarithmic scaling\n",
        "# Setting norm='l2' normalizes the word vectors to have a length of 1 using cosine normalization\n",
        "tfidf_transformer = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=False, sublinear_tf=False)\n",
        "\n",
        "# Transforming feature vectors into TF-IDF representation\n",
        "tfidf = tfidf_transformer.fit_transform(feature_vectors)\n",
        "\n",
        "# Printing the TF-IDF matrix\n",
        "print(tfidf)\n",
        "\n",
        "# Converting the TF-IDF matrix to a NumPy array\n",
        "tfidfs = tfidf.toarray()\n",
        "\n",
        "# Function to extract top-N feature words for a given document\n",
        "def extract_feature_words(terms, tfidfs, i, n):\n",
        "    tfidf_array = tfidfs[i]\n",
        "    top_n_idx = tfidf_array.argsort()[-n:][::-1]\n",
        "    words = [terms[idx] for idx in top_n_idx]\n",
        "    return words\n",
        "\n",
        "# Looping through the files to extract and print top-20 feature words for each document\n",
        "for i in range(len(files)):\n",
        "    print('------------------------------------------')\n",
        "    feature_words = extract_feature_words(terms, tfidfs, i, 20)\n",
        "    print(files[i])\n",
        "    print(feature_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE54lvGdrw3w",
        "outputId": "b273789d-345f-44c8-f944-7624af001ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Vectors Shape: (7, 30798)\n",
            "Number of Terms: 30798\n",
            "  (0, 30789)\t0.010833078981565639\n",
            "  (0, 30781)\t0.014999750304743077\n",
            "  (0, 30777)\t0.04249884843227921\n",
            "  (0, 30767)\t0.008883279734226925\n",
            "  (0, 30765)\t0.012853646387535644\n",
            "  (0, 30736)\t0.010833078981565639\n",
            "  (0, 30733)\t0.014999750304743077\n",
            "  (0, 30731)\t0.014166282810759738\n",
            "  (0, 30719)\t0.014166282810759738\n",
            "  (0, 30683)\t0.014166282810759738\n",
            "  (0, 30671)\t0.010833078981565639\n",
            "  (0, 30667)\t0.007499875152371539\n",
            "  (0, 30637)\t0.014166282810759738\n",
            "  (0, 30636)\t0.007499875152371539\n",
            "  (0, 30630)\t0.006426823193767822\n",
            "  (0, 30628)\t0.014166282810759738\n",
            "  (0, 30627)\t0.014166282810759738\n",
            "  (0, 30607)\t0.005550075905032825\n",
            "  (0, 30583)\t0.014166282810759738\n",
            "  (0, 30574)\t0.008883279734226925\n",
            "  (0, 30566)\t0.01776655946845385\n",
            "  (0, 30565)\t0.010833078981565639\n",
            "  (0, 30560)\t0.010833078981565639\n",
            "  (0, 30551)\t0.01776655946845385\n",
            "  (0, 30540)\t0.007499875152371539\n",
            "  :\t:\n",
            "  (6, 68)\t0.011274720146208622\n",
            "  (6, 67)\t0.002557503401098938\n",
            "  (6, 66)\t0.005637360073104311\n",
            "  (6, 65)\t0.013251652558834816\n",
            "  (6, 58)\t0.002557503401098938\n",
            "  (6, 56)\t0.004310938002245661\n",
            "  (6, 47)\t0.030920522637281234\n",
            "  (6, 46)\t0.0022086087598058024\n",
            "  (6, 45)\t0.005637360073104311\n",
            "  (6, 40)\t0.005637360073104311\n",
            "  (6, 36)\t0.012932814006736985\n",
            "  (6, 33)\t0.005637360073104311\n",
            "  (6, 32)\t0.005637360073104311\n",
            "  (6, 31)\t0.005637360073104311\n",
            "  (6, 29)\t0.0059690318627740255\n",
            "  (6, 28)\t0.007070061661328904\n",
            "  (6, 25)\t0.005637360073104311\n",
            "  (6, 23)\t0.003535030830664452\n",
            "  (6, 21)\t0.004310938002245661\n",
            "  (6, 18)\t0.011938063725548051\n",
            "  (6, 13)\t0.008953547794161038\n",
            "  (6, 9)\t0.03533774015689284\n",
            "  (6, 6)\t0.005637360073104311\n",
            "  (6, 2)\t0.004310938002245661\n",
            "  (6, 1)\t0.005637360073104311\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/new_york_city/tf-idf/total_period_text/2021-12-05_2022-01-01_new_york.tsv\n",
            "['nye', 'santa', 'tests', 'cases', 'pcr', 'xmas', 'eve', 'playmates', 'omicron', 'citymd', 'mavic', 'vaccinated', 'merry', 'vaxxed', 'dji', 'copyright', 'vikki', 'cdc', 'mandate', 'vaccination']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/new_york_city/tf-idf/total_period_text/2022-01-02_2022-03-05_new_york.tsv\n",
            "['adon', 'magazine', 'partnership', 'snow', 'playmates', 'jorge', 'cric', 'holly', 'ostine', 'har', 'appvisit', 'boom', 'hairrari', 'tourguidestan', 'spades', 'valentines', 'onlinevenmo', 'christian', 'ages', 'donate']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/new_york_city/tf-idf/total_period_text/2022-03-06_2022-05-28_new_york.tsv\n",
            "['adon', 'magazine', 'partnership', 'mfa', 'thesis', 'easter', 'ukraine', 'ecea', 'abortion', 'ladies', 'bathroom', 'conference', 'trent', 'tyson', 'jay', 'mandate', 'mayo', 'rent', 'russian', 'panthers']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/new_york_city/tf-idf/total_period_text/2022-05-29_2022-06-25_new_york.tsv\n",
            "['gelato', 'abortion', 'juneteenth', 'britney', 'imran', 'kele', 'sonny', 'ceremony', 'conference', 'ladies', 'spears', 'bra', 'supporters', 'pals', 'fourth', 'locked', 'playa', 'mets', 'boardwalk', 'package']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/new_york_city/tf-idf/total_period_text/2022-06-26_2022-08-27_new_york.tsv\n",
            "['taiwan', 'pelosi', 'gelato', 'abortion', 'gyamfua', 'teresa', 'dj', 'speaker', 'supplies', 'nancy', 'whale', 'whales', 'giveaway', 'cooling', 'hosted', 'selling', 'camp', 'saudi', 'marina', 'hoboken']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/new_york_city/tf-idf/total_period_text/2022-08-28_2022-10-01_new_york.tsv\n",
            "['supplies', 'hurricane', 'marcos', 'dj', 'princess', 'backpacks', 'zuccotti', 'sp', 'sept', 'depot', 'comment', 'pt', 'rent', 'cream', 'giveaway', 'dish', 'update', 'harrison', 'mets', 'apt']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/new_york_city/tf-idf/total_period_text/2022-10-02_2022-12-31_new_york.tsv\n",
            "['thanksgiving', 'exile', 'election', 'pt', 'santa', 'comics', 'thankful', 'profile', 'mets', 'feet', 'eve', 'closer', 'flavored', 'wakanda', 'smoke', 'mary', 'racist', 'rgc', 'snow', 'polls']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Los Angeles"
      ],
      "metadata": {
        "id": "2shdGD2VJrE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import re\n",
        "\n",
        "# Function to filter out non-English words\n",
        "def is_english_word(word):\n",
        "    return bool(re.match('^[a-zA-Z]+$', word))\n",
        "\n",
        "# Words to be excluded from TF-IDF calculation\n",
        "exclude_words = ['january', 'jan', 'february', 'feb', 'march', 'mar', 'april', 'apr', 'may','june', 'jun', 'july', 'jul', 'august', 'aug', 'september', 'sep', 'october', 'oct', 'november', 'nov','december', 'dec', 'holiday', 'holidays']\n",
        "\n",
        "# Function to filter out specified words\n",
        "def exclude_specific_words(text):\n",
        "    words = text.split()\n",
        "    return [word for word in words if is_english_word(word) and word.lower() not in exclude_words]\n",
        "\n",
        "# min_df=0.10: Exclude the bottom 10% of low-frequency terms\n",
        "# max_df=0.90: Exclude the top 90% of high-frequency terms\n",
        "# ngram_range=(1, 2): Include phrases of up to 2 characters\n",
        "count_vectorizer = CountVectorizer(stop_words='english', input='filename', ngram_range=(1, 1), max_df=0.99)\n",
        "\n",
        "# Corrected path for the files\n",
        "base_path = '/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/los_angeles/tf-idf/total_period_text/'\n",
        "files = [base_path + content for content in sorted(os.listdir(base_path))]\n",
        "\n",
        "# Filter out non-English words and specified words from the features\n",
        "count_vectorizer.set_params(tokenizer=lambda text: exclude_specific_words(text))\n",
        "\n",
        "feature_vectors = count_vectorizer.fit_transform(files)\n",
        "print(\"Feature Vectors Shape:\", feature_vectors.shape)\n",
        "\n",
        "terms = count_vectorizer.get_feature_names_out()\n",
        "print(\"Number of Terms:\", len(terms))\n",
        "\n",
        "# Setting sublinear_tf to True results in logarithmic scaling\n",
        "# Setting norm='l2' normalizes the word vectors to have a length of 1 using cosine normalization\n",
        "tfidf_transformer = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=False, sublinear_tf=False)\n",
        "\n",
        "# Transforming feature vectors into TF-IDF representation\n",
        "tfidf = tfidf_transformer.fit_transform(feature_vectors)\n",
        "\n",
        "# Printing the TF-IDF matrix\n",
        "print(tfidf)\n",
        "\n",
        "# Converting the TF-IDF matrix to a NumPy array\n",
        "tfidfs = tfidf.toarray()\n",
        "\n",
        "# Function to extract top-N feature words for a given document\n",
        "def extract_feature_words(terms, tfidfs, i, n):\n",
        "    tfidf_array = tfidfs[i]\n",
        "    top_n_idx = tfidf_array.argsort()[-n:][::-1]\n",
        "    words = [terms[idx] for idx in top_n_idx]\n",
        "    return words\n",
        "\n",
        "# Looping through the files to extract and print top-20 feature words for each document\n",
        "for i in range(len(files)):\n",
        "    print('------------------------------------------')\n",
        "    feature_words = extract_feature_words(terms, tfidfs, i, 20)\n",
        "    print(files[i])\n",
        "    print(feature_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTNSkpzUv8Vv",
        "outputId": "4612928b-c311-413c-c532-27bafe15c777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Vectors Shape: (8, 27485)\n",
            "Number of Terms: 27485\n",
            "  (0, 27475)\t0.015202540127270242\n",
            "  (0, 27470)\t0.015202540127270242\n",
            "  (0, 27465)\t0.015202540127270242\n",
            "  (0, 27463)\t0.011192001048667065\n",
            "  (0, 27460)\t0.005596000524333533\n",
            "  (0, 27450)\t0.015202540127270242\n",
            "  (0, 27436)\t0.007257091540257154\n",
            "  (0, 27421)\t0.015202540127270242\n",
            "  (0, 27404)\t0.014514183080514309\n",
            "  (0, 27393)\t0.011780621677466342\n",
            "  (0, 27386)\t0.015202540127270242\n",
            "  (0, 27374)\t0.006357009254001286\n",
            "  (0, 27372)\t0.007257091540257154\n",
            "  (0, 27368)\t0.015202540127270242\n",
            "  (0, 27358)\t0.00835870322766244\n",
            "  (0, 27355)\t0.015202540127270242\n",
            "  (0, 27343)\t0.04476800419466826\n",
            "  (0, 27341)\t0.015202540127270242\n",
            "  (0, 27333)\t0.007257091540257154\n",
            "  (0, 27322)\t0.00977892770380519\n",
            "  (0, 27310)\t0.030405080254540483\n",
            "  (0, 27291)\t0.015202540127270242\n",
            "  (0, 27276)\t0.015202540127270242\n",
            "  (0, 27261)\t0.015202540127270242\n",
            "  (0, 27259)\t0.011780621677466342\n",
            "  :\t:\n",
            "  (7, 109)\t0.005200864248250507\n",
            "  (7, 108)\t0.020285624246143767\n",
            "  (7, 102)\t0.006265453223532718\n",
            "  (7, 98)\t0.005200864248250507\n",
            "  (7, 94)\t0.005200864248250507\n",
            "  (7, 93)\t0.0038596407582927503\n",
            "  (7, 83)\t0.004445526349639505\n",
            "  (7, 80)\t0.005952398860433822\n",
            "  (7, 76)\t0.008928598290650732\n",
            "  (7, 75)\t0.005200864248250507\n",
            "  (7, 70)\t0.006265453223532718\n",
            "  (7, 68)\t0.020833396011518374\n",
            "  (7, 60)\t0.014880997151084555\n",
            "  (7, 54)\t0.0033809373743572944\n",
            "  (7, 45)\t0.00808538009742593\n",
            "  (7, 42)\t0.008928598290650732\n",
            "  (7, 41)\t0.004445526349639505\n",
            "  (7, 35)\t0.004445526349639505\n",
            "  (7, 31)\t0.005200864248250507\n",
            "  (7, 25)\t0.004445526349639505\n",
            "  (7, 24)\t0.004445526349639505\n",
            "  (7, 22)\t0.007719281516585501\n",
            "  (7, 16)\t0.01690468687178647\n",
            "  (7, 8)\t0.00808538009742593\n",
            "  (7, 0)\t0.006761874748714589\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/los_angeles/tf-idf/total_period_text/2021-12-05_2022-01-01_los_angeles.tsv\n",
            "['shooky', 'omicron', 'mang', 'xmas', 'rain', 'nye', 'tested', 'eve', 'vaccinated', 'pickup', 'folks', 'moon', 'colours', 'omarion', 'astound', 'merry', 'train', 'incredible', 'pass', 'degrees']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/los_angeles/tf-idf/total_period_text/2022-01-02_2022-02-12_los_angeles.tsv\n",
            "['meye', 'spotify', 'launch', 'vibration', 'return', 'colors', 'reach', 'train', 'katelyn', 'seacoast', 'mallone', 'itunes', 'apparel', 'folks', 'crystals', 'rams', 'dave', 'charge', 'plessy', 'cesspool']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/los_angeles/tf-idf/total_period_text/2022-02-13_2022-03-19_los_angeles.tsv\n",
            "['drumz', 'ikeboy', 'corey', 'chorus', 'braves', 'platforms', 'speed', 'putin', 'pickup', 'return', 'verse', 'brodie', 'access', 'lyrics', 'incredible', 'mafeking', 'volleyball', 'utilise', 'goldenvoice', 'idle']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/los_angeles/tf-idf/total_period_text/2022-03-20_2022-04-02_los_angeles.tsv\n",
            "['anaheim', 'katani', 'axie', 'mace', 'wednesdays', 'mombasa', 'syokimau', 'rd', 'gig', 'container', 'whatsapp', 'houdini', 'creature', 'rain', 'speed', 'sales', 'ceremony', 'arcadia', 'yard', 'uws']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/los_angeles/tf-idf/total_period_text/2022-04-03_2022-06-11_los_angeles.tsv\n",
            "['gun', 'train', 'demand', 'mass', 'lots', 'easter', 'stand', 'anaheim', 'responders', 'dodger', 'assault', 'memorial', 'finding', 'changed', 'grocery', 'carry', 'return', 'folks', 'security', 'kevin']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/los_angeles/tf-idf/total_period_text/2022-06-12_2022-07-30_los_angeles.tsv\n",
            "['kbla', 'abortion', 'kele', 'tumi', 'calhope', 'inquiries', 'usc', 'alex', 'folks', 'lots', 'train', 'rights', 'sinners', 'trew', 'session', 'episode', 'page', 'return', 'arcade', 'pool']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/los_angeles/tf-idf/total_period_text/2022-07-31_2022-11-05_los_angeles.tsv\n",
            "['halloween', 'return', 'lots', 'train', 'sinners', 'folks', 'stand', 'sharing', 'bday', 'sept', 'agree', 'kick', 'incredible', 'spooky', 'universal', 'jr', 'joslyn', 'heat', 'million', 'pelosi']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/los_angeles/tf-idf/total_period_text/2022-11-06_2022-12-31_los_angeles.tsv\n",
            "['thanksgiving', 'xmas', 'return', 'folks', 'rain', 'lots', 'picture', 'code', 'oregon', 'stub', 'merry', 'mother', 'sinners', 'argentina', 'roadtrip', 'naomi', 'episode', 'luck', 'incredible', 'shower']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chicago"
      ],
      "metadata": {
        "id": "vxTQp8SbJuaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import re\n",
        "\n",
        "# Function to filter out non-English words\n",
        "def is_english_word(word):\n",
        "    return bool(re.match('^[a-zA-Z]+$', word))\n",
        "\n",
        "# Words to be excluded from TF-IDF calculation\n",
        "exclude_words = ['january', 'jan', 'february', 'feb', 'march', 'mar', 'april', 'apr', 'may','june', 'jun', 'july', 'jul', 'august', 'aug', 'september', 'sep', 'october', 'oct', 'november', 'nov','december', 'dec', 'holiday', 'holidays']\n",
        "\n",
        "# Function to filter out specified words\n",
        "def exclude_specific_words(text):\n",
        "    words = text.split()\n",
        "    return [word for word in words if is_english_word(word) and word.lower() not in exclude_words]\n",
        "\n",
        "# min_df=0.10: Exclude the bottom 10% of low-frequency terms\n",
        "# max_df=0.90: Exclude the top 90% of high-frequency terms\n",
        "# ngram_range=(1, 2): Include phrases of up to 2 characters\n",
        "count_vectorizer = CountVectorizer(stop_words='english', input='filename', ngram_range=(1, 1), max_df=0.99)\n",
        "\n",
        "# Corrected path for the files\n",
        "base_path = '/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/tf-idf/total_period_text/'\n",
        "files = [base_path + content for content in sorted(os.listdir(base_path))]\n",
        "\n",
        "# Filter out non-English words and specified words from the features\n",
        "count_vectorizer.set_params(tokenizer=lambda text: exclude_specific_words(text))\n",
        "\n",
        "feature_vectors = count_vectorizer.fit_transform(files)\n",
        "print(\"Feature Vectors Shape:\", feature_vectors.shape)\n",
        "\n",
        "terms = count_vectorizer.get_feature_names_out()\n",
        "print(\"Number of Terms:\", len(terms))\n",
        "\n",
        "# Setting sublinear_tf to True results in logarithmic scaling\n",
        "# Setting norm='l2' normalizes the word vectors to have a length of 1 using cosine normalization\n",
        "tfidf_transformer = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=False, sublinear_tf=False)\n",
        "\n",
        "# Transforming feature vectors into TF-IDF representation\n",
        "tfidf = tfidf_transformer.fit_transform(feature_vectors)\n",
        "\n",
        "# Printing the TF-IDF matrix\n",
        "print(tfidf)\n",
        "\n",
        "# Converting the TF-IDF matrix to a NumPy array\n",
        "tfidfs = tfidf.toarray()\n",
        "\n",
        "# Function to extract top-N feature words for a given document\n",
        "def extract_feature_words(terms, tfidfs, i, n):\n",
        "    tfidf_array = tfidfs[i]\n",
        "    top_n_idx = tfidf_array.argsort()[-n:][::-1]\n",
        "    words = [terms[idx] for idx in top_n_idx]\n",
        "    return words\n",
        "\n",
        "# Looping through the files to extract and print top-20 feature words for each document\n",
        "for i in range(len(files)):\n",
        "    print('------------------------------------------')\n",
        "    feature_words = extract_feature_words(terms, tfidfs, i, 20)\n",
        "    print(files[i])\n",
        "    print(feature_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "696VZp9WxCev",
        "outputId": "7c7a486e-1d30-4c11-ef95-763c7806f9e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Vectors Shape: (11, 19519)\n",
            "Number of Terms: 19519\n",
            "  (0, 19512)\t0.015476798503663507\n",
            "  (0, 19510)\t0.025679850084100737\n",
            "  (0, 19496)\t0.012254099186825892\n",
            "  (0, 19485)\t0.018109187075582317\n",
            "  (0, 19480)\t0.01751246759485683\n",
            "  (0, 19467)\t0.018109187075582317\n",
            "  (0, 19465)\t0.018109187075582317\n",
            "  (0, 19449)\t0.0063990112980694715\n",
            "  (0, 19443)\t0.018109187075582317\n",
            "  (0, 19441)\t0.007026739599205371\n",
            "  (0, 19433)\t0.007738399251831753\n",
            "  (0, 19420)\t0.007738399251831753\n",
            "  (0, 19413)\t0.018109187075582317\n",
            "  (0, 19404)\t0.018109187075582317\n",
            "  (0, 19397)\t0.007026739599205371\n",
            "  (0, 19391)\t0.014415037916790002\n",
            "  (0, 19388)\t0.009531638357077925\n",
            "  (0, 19384)\t0.0063990112980694715\n",
            "  (0, 19383)\t0.010720888757997687\n",
            "  (0, 19379)\t0.014415037916790002\n",
            "  (0, 19377)\t0.014415037916790002\n",
            "  (0, 19371)\t0.018109187075582317\n",
            "  (0, 19365)\t0.03199505649034736\n",
            "  (0, 19351)\t0.018109187075582317\n",
            "  (0, 19343)\t0.014415037916790002\n",
            "  :\t:\n",
            "  (10, 84)\t0.011339966802819498\n",
            "  (10, 82)\t0.0072894644536043735\n",
            "  (10, 80)\t0.00857491888141128\n",
            "  (10, 78)\t0.010772417740264908\n",
            "  (10, 77)\t0.00857491888141128\n",
            "  (10, 72)\t0.010772417740264908\n",
            "  (10, 68)\t0.00857491888141128\n",
            "  (10, 67)\t0.01141953350083152\n",
            "  (10, 64)\t0.0072894644536043735\n",
            "  (10, 60)\t0.00380651116694384\n",
            "  (10, 58)\t0.010772417740264908\n",
            "  (10, 54)\t0.00380651116694384\n",
            "  (10, 52)\t0.010772417740264908\n",
            "  (10, 47)\t0.005669983401409749\n",
            "  (10, 45)\t0.010772417740264908\n",
            "  (10, 42)\t0.010772417740264908\n",
            "  (10, 36)\t0.004179921163704022\n",
            "  (10, 27)\t0.006377420022557651\n",
            "  (10, 24)\t0.00857491888141128\n",
            "  (10, 20)\t0.014578928907208747\n",
            "  (10, 17)\t0.006377420022557651\n",
            "  (10, 11)\t0.0072894644536043735\n",
            "  (10, 10)\t0.010772417740264908\n",
            "  (10, 1)\t0.010772417740264908\n",
            "  (10, 0)\t0.0072894644536043735\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/tf-idf/total_period_text/2021-12-12_2022-01-08_chicago.tsv\n",
            "['christmas', 'testing', 'vaccinated', 'vaxxed', 'winter', 'merry', 'tested', 'teachers', 'tests', 'masks', 'test', 'cdc', 'send', 'santa', 'mild', 'nye', 'men', 'unvaccinated', 'sew', 'vaccine']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/tf-idf/total_period_text/2022-01-09_2022-02-19_chicago.tsv\n",
            "['snow', 'polio', 'valentines', 'n', 'mae', 'quite', 'vaccine', 'lens', 'strap', 'rethink', 'baton', 'starry', 'appointment', 'okay', 'west', 'remote', 'lives', 'mental', 'degrees', 'jewel']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/tf-idf/total_period_text/2022-02-20_2022-03-26_chicago.tsv\n",
            "['ukraine', 'creature', 'ukrainian', 'women', 'session', 'uppf', 'group', 'required', 'vaccine', 'masks', 'fucken', 'ifc', 'portlandia', 'merit', 'daylight', 'sessions', 'rest', 'n', 'taken', 'bout']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/tf-idf/total_period_text/2022-03-27_2022-05-14_chicago.tsv\n",
            "['orvieto', 'pauline', 'uppf', 'pagbalik', 'ni', 'fralaine', 'writers', 'wines', 'nevada', 'celebrate', 'dress', 'easter', 'group', 'wine', 'contributing', 'downtown', 'awesome', 'project', 'coach', 'cinco']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/tf-idf/total_period_text/2022-05-15_2022-05-28_chicago.tsv\n",
            "['nra', 'kardashian', 'kourtney', 'send', 'lee', 'government', 'tragedy', 'travis', 'robb', 'families', 'shooter', 'assault', 'shootings', 'ok', 'building', 'bringing', 'politicians', 'die', 'sending', 'dare']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/tf-idf/total_period_text/2022-05-29_2022-06-11_chicago.tsv\n",
            "['national', 'women', 'final', 'logical', 'bush', 'solution', 'event', 'tourists', 'corps', 'alexander', 'normandie', 'grounds', 'laithyn', 'br', 'stomping', 'enosis', 'valuables', 'buchanan', 'wandered', 'owen']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/tf-idf/total_period_text/2022-06-12_2022-07-09_chicago.tsv\n",
            "['women', 'abortion', 'highland', 'harris', 'tamil', 'celebrate', 'tornado', 'twitter', 'session', 'send', 'archer', 'quite', 'cpdd', 'kamala', 'cheryl', 'largest', 'parade', 'dogs', 'rest', 'song']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/tf-idf/total_period_text/2022-07-10_2022-08-20_chicago.tsv\n",
            "['bash', 'supplies', 'finkl', 'saudi', 'group', 'biden', 'bag', 'faculty', 'lee', 'jaz', 'americans', 'sponsors', 'art', 'doors', 'dress', 'dance', 'eating', 'families', 'monkeypox', 'cute']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/tf-idf/total_period_text/2022-08-21_2022-10-01_chicago.tsv\n",
            "['bark', 'tickets', 'birthday', 'dance', 'dress', 'event', 'n', 'hc', 'downtown', 'celebrate', 'west', 'platforms', 'rest', 'eating', 'women', 'wolves', 'art', 'quite', 'send', 'okay']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/tf-idf/total_period_text/2022-10-02_2022-11-12_chicago.tsv\n",
            "['halloween', 'dailey', 'juniors', 'sign', 'visiting', 'encouraged', 'hermanos', 'fascism', 'attend', 'vote', 'evolve', 'seniors', 'goldfish', 'understands', 'paper', 'costume', 'repair', 'awesome', 'golf', 'pawn']\n",
            "------------------------------------------\n",
            "/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/tf-idf/total_period_text/2022-11-13_2022-12-31_chicago.tsv\n",
            "['christmas', 'thanksgiving', 'santa', 'winter', 'merry', 'snow', 'tickets', 'gift', 'storm', 'joi', 'pegging', 'blackout', 'offer', 'fr', 'festive', 'blizzard', 'footworship', 'hermanos', 'tree', 'southwest']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RraL8J5Zxck4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}