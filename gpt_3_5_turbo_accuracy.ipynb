{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRVNhrFAIYK/5ItYJyIHzP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/covid19-twitter-usa-restoring/blob/main/gpt_3_5_turbo_accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-W6uhT9d17c"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade openai\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# https://community.openai.com/t/google-colab-fine-tuning-error/5917\n",
        "# Add Open AI API Key"
      ],
      "metadata": {
        "id": "xWhIU4Mkd-Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-3.5 Turbo Fine-tuning model"
      ],
      "metadata": {
        "id": "kSjfZ3A-7nlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# Function to read data from TSV file using pandas\n",
        "def read_tsv(file_path):\n",
        "    data = pd.read_table(file_path, names=['content', 'label'], dtype='object', engine='python')\n",
        "    return data\n",
        "\n",
        "# Test data file path (Replace with your Google Drive directory and file)\n",
        "file_path = '/content/drive/My Drive/covid-twitter-usa-normal/data/training_data/gpt-3.5/test_data_2021_shuffle_majority_vote_gpt3.5.tsv'\n",
        "\n",
        "# Read data from TSV file using pandas\n",
        "test_data = read_tsv(file_path)\n",
        "\n",
        "# Function to classify sentiment using OpenAI API\n",
        "def classify_sentiment(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"ft:gpt-3.5-turbo-1106:university-of-tsukuba::8RcZEQZm\",\n",
        "        messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a sentiment classifier of tweets from citizens of a large US city collected during the COVID-19 pandemic. Classify into three values: 0 for positive, 1 for neutral, and 2 for negative. Positive sentiment include: admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fantastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud, rejoice, relief, respect, satisfactorily, sensational, super, terrific, thank, vivid, wise, wonderful, zest, expectations and etc. Negative sentiment include: abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit, defective, disappointment, embarrass, fake, fear, filthy, fool, guilt, hate, idiot, inflict, lazy, miserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly, vile, wicked, and etc. Neutral sentiment: neither positive or negative, such as text without sentiment, stating a fact, question, news article, advertisement, solicitation, request, quote, unintelligible text, and etc. When the sentiment is mixed, such as both joy and sadness, use your judgment to choose the stronger emotion.\"},\n",
        "          {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"Pridiction: \" + response.choices[0].message.content)\n",
        "    print(\"\")\n",
        "    try:\n",
        "        return int(response.choices[0].message.content)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Unable to convert '{response.choices[0].message.content}' to int. Skipping this prediction.\")\n",
        "        return None\n",
        "\n",
        "# Lists to store true and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Calculate accuracy with a 2-second interval between classify_sentiment calls\n",
        "for _, example in test_data.iterrows():\n",
        "    text = example['content']\n",
        "    true_label = int(example['label'])\n",
        "    print(\"Ground Truth: \" + example['label'])\n",
        "\n",
        "    # Call classify_sentiment with a 2-second interval (Note: This code won't run as OpenAI API access is not provided)\n",
        "    sleep(2)\n",
        "    predicted_label = classify_sentiment(text)\n",
        "\n",
        "    # Skip the prediction if it is None\n",
        "    if predicted_label is not None:\n",
        "        predicted_labels.append(predicted_label)\n",
        "        true_labels.append(true_label)\n",
        "\n",
        "# Calculate and display accuracy, recall, precision, and F1 score\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels, average=None)\n",
        "precision = precision_score(true_labels, predicted_labels, average=None)\n",
        "f1 = f1_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "# Display classification report and confusion matrix\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(true_labels, predicted_labels))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "# Display metrics for each class and macro/micro averages\n",
        "macro_avg = precision.mean(), recall.mean(), f1.mean()\n",
        "micro_avg = precision.sum() / 3, recall.sum() / 3, f1.sum() / 3\n",
        "\n",
        "# Display metrics for each class and macro/micro averages\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "for i in range(3):\n",
        "    print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "print(f\"| Macro Average|    {accuracy:.2f}   |   {recall.mean():.2f}   |   {precision.mean():.2f}   |   {f1.mean():.2f}   |\")\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "print(f\"| Micro Average|    {accuracy:.2f}   |   {recall.sum()/3:.2f}   |   {precision.sum()/3:.2f}   |   {f1.sum()/3:.2f}   |\")\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n"
      ],
      "metadata": {
        "id": "OBC0LreNvBTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.81      0.89      0.85       270\n",
        "           1       0.74      0.57      0.64       214\n",
        "           2       0.83      0.91      0.86       264\n",
        "\n",
        "    accuracy                           0.80       748\n",
        "   macro avg       0.79      0.79      0.79       748\n",
        "weighted avg       0.80      0.80      0.80       748\n",
        "\n",
        "\n",
        "Confusion Matrix:\n",
        "[[239  24   7]\n",
        " [ 49 122  43]\n",
        " [  6  19 239]]"
      ],
      "metadata": {
        "id": "anGBEhJ0j6_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-3.5 Turbo Non-fine-tuning model - Rich Backgourd"
      ],
      "metadata": {
        "id": "VNLTBjLq0Pip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# Function to read data from TSV file using pandas\n",
        "def read_tsv(file_path):\n",
        "    data = pd.read_table(file_path, names=['content', 'label'], dtype='object', engine='python')\n",
        "    return data\n",
        "\n",
        "# Test data file path (Replace with your Google Drive directory and file)\n",
        "file_path = '/content/drive/My Drive/covid-twitter-usa-normal/data/training_data/gpt-3.5/test_data_2021_shuffle_majority_vote_gpt3.5.tsv'\n",
        "\n",
        "# Read data from TSV file using pandas\n",
        "test_data = read_tsv(file_path)\n",
        "\n",
        "# Function to classify sentiment using OpenAI API\n",
        "def classify_sentiment(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a sentiment classifier of tweets from citizens of a large US city collected during the COVID-19 pandemic. Classify into three values: 0 for positive, 1 for neutral, and 2 for negative. Please do not answer anything other than 0, 1, or 2. Positive sentiment include: admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fantastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud, rejoice, relief, respect, satisfactorily, sensational, super, terrific, thank, vivid, wise, wonderful, zest, expectations and etc. Negative sentiment include: abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit, defective, disappointment, embarrass, fake, fear, filthy, fool, guilt, hate, idiot, inflict, lazy, miserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly, vile, wicked, and etc. Neutral sentiment: neither positive or negative, such as text without sentiment, stating a fact, question, news article, advertisement, solicitation, request, quote, unintelligible text, and etc. When the sentiment is mixed, such as both joy and sadness, use your judgment to choose the stronger emotion.\"},\n",
        "          {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"Pridiction: \" + response.choices[0].message.content)\n",
        "    print(\"\")\n",
        "    try:\n",
        "        return int(response.choices[0].message.content)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Unable to convert '{response.choices[0].message.content}' to int. Skipping this prediction.\")\n",
        "        return None\n",
        "\n",
        "# Lists to store true and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Calculate accuracy with a 2-second interval between classify_sentiment calls\n",
        "for _, example in test_data.iterrows():\n",
        "    text = example['content']\n",
        "    true_label = int(example['label'])\n",
        "    print(\"Ground Truth: \" + example['label'])\n",
        "\n",
        "    # Call classify_sentiment with a 2-second interval (Note: This code won't run as OpenAI API access is not provided)\n",
        "    sleep(2)\n",
        "    predicted_label = classify_sentiment(text)\n",
        "\n",
        "    # Skip the prediction if it is None\n",
        "    if predicted_label is not None:\n",
        "        predicted_labels.append(predicted_label)\n",
        "        true_labels.append(true_label)\n",
        "\n",
        "# Calculate and display accuracy, recall, precision, and F1 score\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels, average=None)\n",
        "precision = precision_score(true_labels, predicted_labels, average=None)\n",
        "f1 = f1_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "# Display classification report and confusion matrix\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(true_labels, predicted_labels))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "# Display metrics for each class and macro/micro averages\n",
        "macro_avg = precision.mean(), recall.mean(), f1.mean()\n",
        "micro_avg = precision.sum() / 3, recall.sum() / 3, f1.sum() / 3\n",
        "\n",
        "# Display metrics for each class and macro/micro averages\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "for i in range(3):\n",
        "    print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "print(f\"| Macro Average|    {accuracy:.2f}   |   {recall.mean():.2f}   |   {precision.mean():.2f}   |   {f1.mean():.2f}   |\")\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "print(f\"| Micro Average|    {accuracy:.2f}   |   {recall.sum()/3:.2f}   |   {precision.sum()/3:.2f}   |   {f1.sum()/3:.2f}   |\")\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n"
      ],
      "metadata": {
        "id": "H4W7J4VsNgYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.80      0.83      0.81       270\n",
        "           1       0.73      0.48      0.58       214\n",
        "           2       0.73      0.90      0.81       265\n",
        "\n",
        "    accuracy                           0.75       749\n",
        "   macro avg       0.75      0.74      0.73       749\n",
        "weighted avg       0.75      0.75      0.74       749\n",
        "\n",
        "\n",
        "Confusion Matrix:\n",
        "[[224  19  27]\n",
        " [ 50 102  62]\n",
        " [  7  19 239]]"
      ],
      "metadata": {
        "id": "-X7mKeuYj_bH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-3.5 Turbo Non-fine-tuning model - Minimum Backgroud"
      ],
      "metadata": {
        "id": "GhfKo9aCDBqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# Function to read data from TSV file using pandas\n",
        "def read_tsv(file_path):\n",
        "    data = pd.read_table(file_path, names=['content', 'label'], dtype='object', engine='python')\n",
        "    return data\n",
        "\n",
        "# Test data file path (Replace with your Google Drive directory and file)\n",
        "file_path = '/content/drive/My Drive/covid-twitter-usa-normal/data/training_data/gpt-3.5/test_data_2021_shuffle_majority_vote_gpt3.5.tsv'\n",
        "\n",
        "# Read data from TSV file using pandas\n",
        "test_data = read_tsv(file_path)\n",
        "\n",
        "# Function to classify sentiment using OpenAI API\n",
        "def classify_sentiment(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a sentiment classifier of tweets from citizens of a large US city collected during the COVID-19 pandemic. Classify into three values: 0 for positive, 1 for neutral, and 2 for negative. Please do not answer anything other than 0, 1, or 2.\"},\n",
        "          {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"Pridiction: \" + response.choices[0].message.content)\n",
        "    print(\"\")\n",
        "    try:\n",
        "        return int(response.choices[0].message.content)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Unable to convert '{response.choices[0].message.content}' to int. Skipping this prediction.\")\n",
        "        return None\n",
        "\n",
        "# Lists to store true and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Calculate accuracy with a 2-second interval between classify_sentiment calls\n",
        "for _, example in test_data.iterrows():\n",
        "    text = example['content']\n",
        "    true_label = int(example['label'])\n",
        "    print(\"Ground Truth: \" + example['label'])\n",
        "\n",
        "    # Call classify_sentiment with a 2-second interval (Note: This code won't run as OpenAI API access is not provided)\n",
        "    sleep(2)\n",
        "    predicted_label = classify_sentiment(text)\n",
        "\n",
        "    # Skip the prediction if it is None\n",
        "    if predicted_label is not None:\n",
        "        predicted_labels.append(predicted_label)\n",
        "        true_labels.append(true_label)\n",
        "\n",
        "# Calculate and display accuracy, recall, precision, and F1 score\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels, average=None)\n",
        "precision = precision_score(true_labels, predicted_labels, average=None)\n",
        "f1 = f1_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "# Display classification report and confusion matrix\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(true_labels, predicted_labels))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(true_labels, predicted_labels))\n",
        "\n",
        "# Display metrics for each class and macro/micro averages\n",
        "macro_avg = precision.mean(), recall.mean(), f1.mean()\n",
        "micro_avg = precision.sum() / 3, recall.sum() / 3, f1.sum() / 3\n",
        "\n",
        "# Display metrics for each class and macro/micro averages\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "print(\"|   Metric     | Accuracy  |  Recall  | Precision|  F1 Score |\")\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "for i in range(3):\n",
        "    print(f\"| Class {i}      |    {accuracy:.2f}   |   {recall[i]:.2f}   |   {precision[i]:.2f}   |   {f1[i]:.2f}   |\")\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "print(f\"| Macro Average|    {accuracy:.2f}   |   {recall.mean():.2f}   |   {precision.mean():.2f}   |   {f1.mean():.2f}   |\")\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")\n",
        "print(f\"| Micro Average|    {accuracy:.2f}   |   {recall.sum()/3:.2f}   |   {precision.sum()/3:.2f}   |   {f1.sum()/3:.2f}   |\")\n",
        "print(\"+--------------+-----------+----------+----------+----------+\")"
      ],
      "metadata": {
        "id": "7LmMAcPCzFzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.76      0.87      0.81       270\n",
        "           1       0.76      0.32      0.45       214\n",
        "           2       0.71      0.94      0.81       265\n",
        "\n",
        "    accuracy                           0.74       749\n",
        "   macro avg       0.74      0.71      0.69       749\n",
        "weighted avg       0.74      0.74      0.71       749\n",
        "\n",
        "\n",
        "Confusion Matrix:\n",
        "[[234  13  23]\n",
        " [ 65  69  80]\n",
        " [  7   9 249]]"
      ],
      "metadata": {
        "id": "LAZDV3OTkCom"
      }
    }
  ]
}