{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1yijpZKsWWpc1AfUewrPf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/covid19-twitter-usa-restoring/blob/main/sentiment_classifier_gpt_3_5_turbo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTLXx8NLbWGd"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade openai\n",
        "!pip install --upgrade gspread"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQCCqugsbpmw",
        "outputId": "10eb78c3-71db-43da-de06-9170eac724ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# https://community.openai.com/t/google-colab-fine-tuning-error/5917\n"
      ],
      "metadata": {
        "id": "lMHcrNDsbq79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New York City"
      ],
      "metadata": {
        "id": "AlkC1q5DjOyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gspread\n",
        "import pandas as pd\n",
        "\n",
        "# Input Files\n",
        "tsv_files=os.listdir('/content/drive/My Drive/covid-twitter-usa-normal/data/collection_data/new_york_city/3/1/')\n",
        "# tsv_files=os.listdir('/content/drive/My Drive/covid-twitter-usa-normal/data/collection_data/new_york_city/1/work/')\n",
        "\n",
        "# Sort the file names\n",
        "tsv_files_sorted = sorted(tsv_files)\n",
        "\n",
        "# Output File\n",
        "url = \"https://docs.google.com/spreadsheets/d/1_A7eljHvSSeqpDUEDrMZFU44-durXx1lXoK8k4mAjh0/edit#gid=314682145\"\n",
        "sheet_name = '3'"
      ],
      "metadata": {
        "id": "D3wNzY8abwjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "number = 0\n",
        "limit_control_no = 0\n",
        "\n",
        "workbook = gc.open_by_url(url)\n",
        "worksheet = workbook.worksheet(sheet_name)\n",
        "header = ['file name', 'tweets', 'positive', 'neutral', 'negative', 'score']\n",
        "worksheet.append_row(header)\n",
        "\n",
        "# Function to classify sentiment using OpenAI API\n",
        "def classify_sentiment(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"ft:gpt-3.5-turbo-1106:university-of-tsukuba::8RcZEQZm\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a sentiment classifier of tweets from citizens of a large US city collected during the COVID-19 pandemic. Classify into three values: 0 for positive, 1 for neutral, and 2 for negative. Positive sentiment includes: admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fantastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud, rejoice, relief, respect, satisfactorily, sensational, super, terrific, thank, vivid, wise, wonderful, zest, expectations and etc. Negative sentiment includes: abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit, defective, disappointment, embarrass, fake, fear, filthy, fool, guilt, hate, idiot, inflict, lazy, miserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly, vile, wicked, and etc. Neutral sentiment: neither positive or negative, such as text without sentiment, stating a fact, question, news article, advertisement, solicitation, request, quote, unintelligible text, and etc. When the sentiment is mixed, such as both joy and sadness, use your judgment to choose the stronger emotion.\"},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ],\n",
        "        timeout=60  # Timeout set to 60 seconds\n",
        "    )\n",
        "\n",
        "    print(\"Prediction: \" + response.choices[0].message.content)\n",
        "    print(\"\")\n",
        "    try:\n",
        "        return int(response.choices[0].message.content)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Unable to convert '{response.choices[0].message.content}' to int. Skipping this prediction.\")\n",
        "        return None\n",
        "\n",
        "for h in range(len(tsv_files)):\n",
        "\n",
        "    print(str(tsv_files[h]))\n",
        "\n",
        "    columns = ['id', 'author_id', 'tweet', 'index']\n",
        "    df_output = pd.DataFrame(columns=columns)\n",
        "\n",
        "    sum_num = 0\n",
        "    pos_num = 0\n",
        "    neu_num = 0\n",
        "    neg_num = 0\n",
        "    other_num = 0\n",
        "    score = 0\n",
        "    number = 0\n",
        "    limit_control_no = 0\n",
        "    sentiment_score = 0\n",
        "    sentiment_index = 0\n",
        "    tweets_number = 0\n",
        "\n",
        "    # â˜†\n",
        "    df_input = pd.read_table('/content/drive/My Drive/covid-twitter-usa-normal/data/collection_data/new_york_city/3/1/' + str(tsv_files[h]), names=('author_id', 'time', 'id', 'text', 'place'))\n",
        "    print(\"The number of line:\", len(df_input))\n",
        "    # Duplicate Text Column\n",
        "    df_input = df_input.drop_duplicates(subset='text')\n",
        "    print(\"The number of line after duplication:\", len(df_input))\n",
        "    # Rate Limit Counterplan\n",
        "    print('Wait for 3 seconds')\n",
        "    time.sleep(3)\n",
        "\n",
        "    for index, item in df_input.iterrows():\n",
        "\n",
        "        print(index)\n",
        "        text = item['text']\n",
        "        print(text)\n",
        "\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        max_retries = 3\n",
        "        retries = 0\n",
        "\n",
        "        while retries < max_retries:\n",
        "            try:\n",
        "                predicted_label = classify_sentiment(text)\n",
        "                break  # If successful, exit the loop\n",
        "            except Exception as e:\n",
        "                retries += 1\n",
        "                print(f\"Error: {str(e)}. Retrying {retries}/{max_retries}...\")  # Fix: Print retry information\n",
        "                if retries == max_retries:\n",
        "                    # Log the error and raise an exception to stop processing\n",
        "                    print(\"Max retries exceeded. Logging the error and stopping the processing.\")\n",
        "                    raise\n",
        "                time.sleep(5)\n",
        "\n",
        "        number += 1\n",
        "        limit_control_no += 1\n",
        "\n",
        "        if predicted_label == 0:\n",
        "            pos_num += 1\n",
        "            score = 0\n",
        "            sum_num += -1\n",
        "        elif predicted_label == 1:\n",
        "            neu_num += 1\n",
        "            score = 1\n",
        "            sum_num += 0\n",
        "        elif predicted_label == 2:\n",
        "            neg_num += 1\n",
        "            score = 2\n",
        "            sum_num += 1\n",
        "        else:\n",
        "            other_num += 1\n",
        "            score = 4\n",
        "\n",
        "        # Rate Limit Counterplan\n",
        "        if limit_control_no == 200:\n",
        "            limit_control_no = 0\n",
        "            print('Wait for 10 seconds')\n",
        "            time.sleep(10)\n",
        "\n",
        "        append_list = [str(item['id']), str(item['author_id']), str(item['text']), str(score)]\n",
        "        df_next = pd.DataFrame([append_list], columns=columns)\n",
        "        df_output = pd.concat([df_output, df_next], ignore_index=True)\n",
        "\n",
        "    sentiment_score = sum_num / number\n",
        "    sentiment_index = 'Sentiment Index: ' + str(sentiment_score)\n",
        "    tweets_number = 'Tweet_Number: ' + str(number)\n",
        "    positive_number = str(pos_num)\n",
        "    neutral_number = str(neu_num)\n",
        "    negative_number = str(neg_num)\n",
        "    file_name = 'File_Name: ' + str(tsv_files[h])\n",
        "    print(file_name + ' ' + tweets_number + ' ' + str(sentiment_index))\n",
        "    result_set = [str(tsv_files[h]), str(number), positive_number, neutral_number, negative_number, str(sentiment_score)]\n",
        "    worksheet.append_row(result_set)\n",
        "\n",
        "    # â˜†\n",
        "    directory = '/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/new_york_city/3/' + str(tsv_files[h])\n",
        "    df_output.to_csv(directory, sep='\\t', index=False, header=False)\n"
      ],
      "metadata": {
        "id": "ty1QJCqcV9eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Los Angeles"
      ],
      "metadata": {
        "id": "Jod1M3o9jVJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gspread\n",
        "import pandas as pd\n",
        "\n",
        "# Input Files\n",
        "tsv_files=os.listdir('/content/drive/My Drive/covid-twitter-usa-normal/data/collection_data/los_angeles/3/1/')\n",
        "# tsv_files=os.listdir('/content/drive/My Drive/covid-twitter-usa-normal/data/collection_data/new_york_city/1/work/')\n",
        "\n",
        "# Sort the file names\n",
        "tsv_files_sorted = sorted(tsv_files)\n",
        "\n",
        "# Output File\n",
        "url = \"https://docs.google.com/spreadsheets/d/1SjTCyR9rMUkPfuJBa1ZR6YBvJl3masUiQU0GJ7JHCNg/edit#gid=314682145\"\n",
        "sheet_name = '3'"
      ],
      "metadata": {
        "id": "hK5lTF4vnvgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "number = 0\n",
        "limit_control_no = 0\n",
        "\n",
        "workbook = gc.open_by_url(url)\n",
        "worksheet = workbook.worksheet(sheet_name)\n",
        "header = ['file name', 'tweets', 'positive', 'neutral', 'negative', 'score']\n",
        "worksheet.append_row(header)\n",
        "\n",
        "# Function to classify sentiment using OpenAI API\n",
        "def classify_sentiment(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"ft:gpt-3.5-turbo-1106:university-of-tsukuba::8RcZEQZm\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a sentiment classifier of tweets from citizens of a large US city collected during the COVID-19 pandemic. Classify into three values: 0 for positive, 1 for neutral, and 2 for negative. Positive sentiment includes: admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fantastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud, rejoice, relief, respect, satisfactorily, sensational, super, terrific, thank, vivid, wise, wonderful, zest, expectations and etc. Negative sentiment includes: abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit, defective, disappointment, embarrass, fake, fear, filthy, fool, guilt, hate, idiot, inflict, lazy, miserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly, vile, wicked, and etc. Neutral sentiment: neither positive or negative, such as text without sentiment, stating a fact, question, news article, advertisement, solicitation, request, quote, unintelligible text, and etc. When the sentiment is mixed, such as both joy and sadness, use your judgment to choose the stronger emotion.\"},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ],\n",
        "        timeout=60  # Timeout set to 60 seconds\n",
        "    )\n",
        "\n",
        "    print(\"Prediction: \" + response.choices[0].message.content)\n",
        "    print(\"\")\n",
        "    try:\n",
        "        return int(response.choices[0].message.content)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Unable to convert '{response.choices[0].message.content}' to int. Skipping this prediction.\")\n",
        "        return None\n",
        "\n",
        "for h in range(len(tsv_files)):\n",
        "\n",
        "    print(str(tsv_files[h]))\n",
        "\n",
        "    columns = ['id', 'author_id', 'tweet', 'index']\n",
        "    df_output = pd.DataFrame(columns=columns)\n",
        "\n",
        "    sum_num = 0\n",
        "    pos_num = 0\n",
        "    neu_num = 0\n",
        "    neg_num = 0\n",
        "    other_num = 0\n",
        "    score = 0\n",
        "    number = 0\n",
        "    limit_control_no = 0\n",
        "    sentiment_score = 0\n",
        "    sentiment_index = 0\n",
        "    tweets_number = 0\n",
        "\n",
        "    # â˜†\n",
        "    df_input = pd.read_table('/content/drive/My Drive/covid-twitter-usa-normal/data/collection_data/los_angeles/3/1/' + str(tsv_files[h]), names=('author_id', 'time', 'id', 'text', 'place'))\n",
        "    print(\"The number of line:\", len(df_input))\n",
        "    # Duplicate Text Column\n",
        "    df_input = df_input.drop_duplicates(subset='text')\n",
        "    print(\"The number of line after duplication:\", len(df_input))\n",
        "    # Rate Limit Counterplan\n",
        "    print('Wait for 3 seconds')\n",
        "    time.sleep(3)\n",
        "\n",
        "    for index, item in df_input.iterrows():\n",
        "\n",
        "        print(index)\n",
        "        text = item['text']\n",
        "        print(text)\n",
        "\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        max_retries = 3\n",
        "        retries = 0\n",
        "\n",
        "        while retries < max_retries:\n",
        "            try:\n",
        "                predicted_label = classify_sentiment(text)\n",
        "                break  # If successful, exit the loop\n",
        "            except Exception as e:\n",
        "                retries += 1\n",
        "                print(f\"Error: {str(e)}. Retrying {retries}/{max_retries}...\")  # Fix: Print retry information\n",
        "                if retries == max_retries:\n",
        "                    # Log the error and raise an exception to stop processing\n",
        "                    print(\"Max retries exceeded. Logging the error and stopping the processing.\")\n",
        "                    raise\n",
        "                time.sleep(5)\n",
        "\n",
        "        number += 1\n",
        "        limit_control_no += 1\n",
        "\n",
        "        if predicted_label == 0:\n",
        "            pos_num += 1\n",
        "            score = 0\n",
        "            sum_num += -1\n",
        "        elif predicted_label == 1:\n",
        "            neu_num += 1\n",
        "            score = 1\n",
        "            sum_num += 0\n",
        "        elif predicted_label == 2:\n",
        "            neg_num += 1\n",
        "            score = 2\n",
        "            sum_num += 1\n",
        "        else:\n",
        "            other_num += 1\n",
        "            score = 4\n",
        "\n",
        "        # Rate Limit Counterplan\n",
        "        if limit_control_no == 200:\n",
        "            limit_control_no = 0\n",
        "            print('Wait for 10 seconds')\n",
        "            time.sleep(10)\n",
        "\n",
        "        append_list = [str(item['id']), str(item['author_id']), str(item['text']), str(score)]\n",
        "        df_next = pd.DataFrame([append_list], columns=columns)\n",
        "        df_output = pd.concat([df_output, df_next], ignore_index=True)\n",
        "\n",
        "    sentiment_score = sum_num / number\n",
        "    sentiment_index = 'Sentiment Index: ' + str(sentiment_score)\n",
        "    tweets_number = 'Tweet_Number: ' + str(number)\n",
        "    positive_number = str(pos_num)\n",
        "    neutral_number = str(neu_num)\n",
        "    negative_number = str(neg_num)\n",
        "    file_name = 'File_Name: ' + str(tsv_files[h])\n",
        "    print(file_name + ' ' + tweets_number + ' ' + str(sentiment_index))\n",
        "    result_set = [str(tsv_files[h]), str(number), positive_number, neutral_number, negative_number, str(sentiment_score)]\n",
        "    worksheet.append_row(result_set)\n",
        "\n",
        "    # â˜†\n",
        "    directory = '/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/los_angeles/3/' + str(tsv_files[h])\n",
        "    df_output.to_csv(directory, sep='\\t', index=False, header=False)\n"
      ],
      "metadata": {
        "id": "bQLoRQGJiLeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chicago"
      ],
      "metadata": {
        "id": "SKK41HIS3isd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gspread\n",
        "import pandas as pd\n",
        "from google.colab import auth\n",
        "\n",
        "# Input Files\n",
        "tsv_directory = '/content/drive/My Drive/covid-twitter-usa-normal/data/collection_data/chicago/3/1/'\n",
        "tsv_files = os.listdir(tsv_directory)\n",
        "\n",
        "# Sort the file names\n",
        "tsv_files_sorted = sorted(tsv_files)\n",
        "\n",
        "# Output File\n",
        "url = \"https://docs.google.com/spreadsheets/d/1RIQ8IZJk-Ooq-OWhO4Bi_AN_uEvlN6SC-QG6wWQKv5w/edit#gid=314682145\"\n",
        "sheet_name = '3'\n",
        "\n",
        "# Output the contents of tsv_files\n",
        "print(\"Contents of tsv_files:\")\n",
        "for file in tsv_files:\n",
        "    print(file)"
      ],
      "metadata": {
        "id": "Vv7AfxEtjb-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "number = 0\n",
        "limit_control_no = 0\n",
        "\n",
        "workbook = gc.open_by_url(url)\n",
        "worksheet = workbook.worksheet(sheet_name)\n",
        "header = ['file name', 'tweets', 'positive', 'neutral', 'negative', 'score']\n",
        "worksheet.append_row(header)\n",
        "\n",
        "# Function to classify sentiment using OpenAI API\n",
        "def classify_sentiment(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"ft:gpt-3.5-turbo-1106:university-of-tsukuba::8RcZEQZm\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a sentiment classifier of tweets from citizens of a large US city collected during the COVID-19 pandemic. Classify into three values: 0 for positive, 1 for neutral, and 2 for negative. Positive sentiment includes: admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fantastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud, rejoice, relief, respect, satisfactorily, sensational, super, terrific, thank, vivid, wise, wonderful, zest, expectations and etc. Negative sentiment includes: abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit, defective, disappointment, embarrass, fake, fear, filthy, fool, guilt, hate, idiot, inflict, lazy, miserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly, vile, wicked, and etc. Neutral sentiment: neither positive or negative, such as text without sentiment, stating a fact, question, news article, advertisement, solicitation, request, quote, unintelligible text, and etc. When the sentiment is mixed, such as both joy and sadness, use your judgment to choose the stronger emotion.\"},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ],\n",
        "        timeout=60  # Timeout set to 60 seconds\n",
        "    )\n",
        "\n",
        "    print(\"Prediction: \" + response.choices[0].message.content)\n",
        "    print(\"\")\n",
        "    try:\n",
        "        return int(response.choices[0].message.content)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Unable to convert '{response.choices[0].message.content}' to int. Skipping this prediction.\")\n",
        "        return None\n",
        "\n",
        "for h in range(len(tsv_files)):\n",
        "\n",
        "    print(str(tsv_files[h]))\n",
        "\n",
        "    columns = ['id', 'author_id', 'tweet', 'index']\n",
        "    df_output = pd.DataFrame(columns=columns)\n",
        "\n",
        "    sum_num = 0\n",
        "    pos_num = 0\n",
        "    neu_num = 0\n",
        "    neg_num = 0\n",
        "    other_num = 0\n",
        "    score = 0\n",
        "    number = 0\n",
        "    limit_control_no = 0\n",
        "    sentiment_score = 0\n",
        "    sentiment_index = 0\n",
        "    tweets_number = 0\n",
        "\n",
        "    # â˜†\n",
        "    df_input = pd.read_table('/content/drive/My Drive/covid-twitter-usa-normal/data/collection_data/chicago/3/1/' + str(tsv_files[h]), names=('author_id', 'time', 'id', 'text', 'place'))\n",
        "    print(\"The number of line:\", len(df_input))\n",
        "    # Duplicate Text Column\n",
        "    df_input = df_input.drop_duplicates(subset='text')\n",
        "    print(\"The number of line after duplication:\", len(df_input))\n",
        "    # Rate Limit Counterplan\n",
        "    print('Wait for 3 seconds')\n",
        "    time.sleep(3)\n",
        "\n",
        "    for index, item in df_input.iterrows():\n",
        "\n",
        "        print(index)\n",
        "        text = item['text']\n",
        "        print(text)\n",
        "\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        max_retries = 3\n",
        "        retries = 0\n",
        "\n",
        "        while retries < max_retries:\n",
        "            try:\n",
        "                predicted_label = classify_sentiment(text)\n",
        "                break  # If successful, exit the loop\n",
        "            except Exception as e:\n",
        "                retries += 1\n",
        "                print(f\"Error: {str(e)}. Retrying {retries}/{max_retries}...\")  # Fix: Print retry information\n",
        "                if retries == max_retries:\n",
        "                    # Log the error and raise an exception to stop processing\n",
        "                    print(\"Max retries exceeded. Logging the error and stopping the processing.\")\n",
        "                    raise\n",
        "                time.sleep(5)\n",
        "\n",
        "        number += 1\n",
        "        limit_control_no += 1\n",
        "\n",
        "        if predicted_label == 0:\n",
        "            pos_num += 1\n",
        "            score = 0\n",
        "            sum_num += -1\n",
        "        elif predicted_label == 1:\n",
        "            neu_num += 1\n",
        "            score = 1\n",
        "            sum_num += 0\n",
        "        elif predicted_label == 2:\n",
        "            neg_num += 1\n",
        "            score = 2\n",
        "            sum_num += 1\n",
        "        else:\n",
        "            other_num += 1\n",
        "            score = 4\n",
        "\n",
        "        # Rate Limit Counterplan\n",
        "        if limit_control_no == 200:\n",
        "            limit_control_no = 0\n",
        "            print('Wait for 10 seconds')\n",
        "            time.sleep(10)\n",
        "\n",
        "        append_list = [str(item['id']), str(item['author_id']), str(item['text']), str(score)]\n",
        "        df_next = pd.DataFrame([append_list], columns=columns)\n",
        "        df_output = pd.concat([df_output, df_next], ignore_index=True)\n",
        "\n",
        "    sentiment_score = sum_num / number\n",
        "    sentiment_index = 'Sentiment Index: ' + str(sentiment_score)\n",
        "    tweets_number = 'Tweet_Number: ' + str(number)\n",
        "    positive_number = str(pos_num)\n",
        "    neutral_number = str(neu_num)\n",
        "    negative_number = str(neg_num)\n",
        "    file_name = 'File_Name: ' + str(tsv_files[h])\n",
        "    print(file_name + ' ' + tweets_number + ' ' + str(sentiment_index))\n",
        "    result_set = [str(tsv_files[h]), str(number), positive_number, neutral_number, negative_number, str(sentiment_score)]\n",
        "    worksheet.append_row(result_set)\n",
        "\n",
        "    # â˜†\n",
        "    directory = '/content/drive/My Drive/covid-twitter-usa-normal/result/gpt-3.5-turbo/chicago/3/' + str(tsv_files[h])\n",
        "    df_output.to_csv(directory, sep='\\t', index=False, header=False)"
      ],
      "metadata": {
        "id": "6biBy5CX3UsO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}