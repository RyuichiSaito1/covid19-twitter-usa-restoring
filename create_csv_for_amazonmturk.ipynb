{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6rGAMS+MAe3VR22RmiYtK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/covid19-twitter-usa-restoring/blob/main/create_csv_for_amazonmturk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ubwLwcB61pb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Specify directory path and file names\n",
        "data_directory = '/content/drive/My Drive/covid-twitter-usa-normal/data/training_data/'\n",
        "data_file_names = ['training_data_2021_shuffle.tsv', 'test_data_2021_shuffle.tsv']\n",
        "output_directory = '/content/drive/My Drive/covid-twitter-usa-normal/data/training_data/AmazonMtuk/'\n",
        "\n",
        "# Initialize an empty DataFrame to store the combined data\n",
        "combined_data = pd.DataFrame(columns=['text', 'score'])\n",
        "\n",
        "# Read and combine the data from the input files\n",
        "for file_name in data_file_names:\n",
        "    file_path = os.path.join(data_directory, file_name)\n",
        "    data = pd.read_table(file_path, names=['text', 'score'], dtype='object', engine='python')\n",
        "    combined_data = combined_data.append(data)\n",
        "\n",
        "# Save the first 300 records as a CSV file with \"text\" header\n",
        "output_first_300_path = os.path.join(output_directory, 'first_300_records.csv')\n",
        "combined_data.head(300)['text'].to_csv(output_first_300_path, index=False, header=True, encoding='utf-8')\n",
        "\n",
        "# Save the rest of the records as a CSV file with \"text\" header\n",
        "output_rest_path = os.path.join(output_directory, 'rest_of_the_records.csv')\n",
        "combined_data.iloc[300:]['text'].to_csv(output_rest_path, index=False, header=True, encoding='utf-8')\n",
        "\n",
        "# Display the top 5 records from the first 300\n",
        "print(\"Top 5 records from the first 300:\")\n",
        "print(combined_data.head(300).head(5))\n",
        "\n",
        "# Display the top 5 records from the rest of the data\n",
        "print(\"Top 5 records from the rest of the data:\")\n",
        "print(combined_data.iloc[300:].head(5))\n",
        "\n",
        "# Output the number of rows in the combined data\n",
        "num_rows = len(combined_data)\n",
        "print(f\"Number of rows in the combined data: {num_rows}\")\n"
      ],
      "metadata": {
        "id": "j6MHgMxGXE-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}